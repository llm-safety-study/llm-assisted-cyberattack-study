# Prompt Behavior Artifacts

This directory contains illustrative artifacts documenting how large language models
responded to different prompt framings during the experiments described in the paper.

The artifacts in this directory are **not attack executions**. Instead, they capture
**model behavior**—including explanations, refusals, redirections, and instructional
walkthroughs generated by the LLMs in response to user queries framed in an academic
and experimental context.

## Purpose of These Artifacts

These images are included to support analysis of:

- How LLMs interpret role-based or contextual prompts
- The types of guidance models are willing to provide
- Differences between refusal, partial assistance, and explanatory responses
- The pedagogical framing adopted by models when operational guidance is restricted

In several cases, the LLMs generated **step-by-step explanatory images or diagrams**
intended to conceptually guide a user through an attack process. These artifacts are
included **as evidence of model behavior**, not as executable instructions.

## Safety and Scope

All artifacts in this directory are intentionally **non-operational**:

- No reusable prompt text is included
- No commands, payloads, or exploit steps are provided
- Images are illustrative and descriptive rather than procedural
- Any instructional content reflects high-level explanation rather than deployment

These artifacts cannot be used to reproduce attacks without substantial external
knowledge and independent technical decisions by a human operator.

## Relationship to the Paper

The paper presents the analytical interpretation of these behaviors.
This directory provides **supporting visual evidence** for claims regarding
LLM response patterns, boundaries, and safety mechanisms.

Artifacts here complement—but do not duplicate—the figures included in the manuscript.

## Ethical Considerations

The inclusion of these artifacts follows responsible disclosure principles.
They are provided to enable transparency and scholarly evaluation of LLM behavior
while avoiding the release of actionable misuse content.
