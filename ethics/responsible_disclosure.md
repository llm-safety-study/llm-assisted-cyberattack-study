# Responsible Disclosure

This repository accompanies an academic study examining the role of large language
models (LLMs) as assistive tools in cyberattack construction. Due to the dual-use
nature of this research, certain materials are intentionally withheld or modified
to prevent misuse.

## Withheld Materials

The following categories of content are not included in this repository:

- Fully functional exploit payloads
- Step-by-step attack execution instructions
- Reusable command sequences that enable real-world exploitation
- Scripts or binaries intended for deployment outside controlled environments
- Instructions for bypassing security controls in live systems

Where relevant, examples are abstracted, sanitized, or replaced with illustrative
artifacts that demonstrate *behavior* rather than operational capability.

## Use of Artifacts

Artifacts included in this repository (e.g., screenshots, logs, prompt excerpts)
serve the following purposes:

- Demonstrating how LLMs respond to different prompt framing strategies
- Illustrating partial attack outcomes in sandboxed environments
- Supporting claims made in the paper regarding model behavior and limitations

All artifacts are non-operational and are provided solely for transparency and
academic analysis.

## Experimental Scope and Consent

All experiments were conducted under the following conditions:

- Systems were owned and controlled by the researchers
- Targets were intentionally vulnerable applications or test programs
- No third-party systems, networks, or users were involved
- No real data or credentials were used

The experiments did not involve unauthorized access to real-world systems.

## Disclosure Philosophy

This work follows established responsible disclosure principles used in security
research:

- Sufficient detail is provided to enable peer review and replication of *context*
- Operational details that could enable harm are omitted
- The focus remains on model behavior, not exploit effectiveness

The authors believe that transparency about AI system behavior is essential for
understanding emerging risks, while acknowledging the responsibility to limit
misuse potential.

## Intended Audience

This repository is intended for:

- Academic reviewers and researchers
- Security practitioners studying AI-assisted threats
- Policymakers and educators examining AI safety implications

It is not intended to serve as a practical attack guide.
